# -*- coding: utf-8 -*-
"""Gesture Recognition System .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17_4ukTHOdXNTVCUHUL2MvKyFw6nitvvR
"""

# Install dependencies
!pip install mediapipe==0.10.9 opencv-python-headless scikit-learn

# Import necessary modules
import cv2
import mediapipe as mp
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from google.colab.patches import cv2_imshow
from IPython.display import clear_output
import time

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(max_num_hands=1)
mp_drawing = mp.solutions.drawing_utils

# Function to extract features from hand landmarks
def extract_features(landmarks):
    if landmarks is None:
        return np.zeros(21 * 3)  # 21 landmarks with x, y, z coordinates
    return np.array([coord for lm in landmarks for coord in (lm.x, lm.y, lm.z)])

# Simulated training data (dummy gestures)
X = []
y = []

for _ in range(100):
    # Simulate wave gesture
    X.append(np.random.rand(21 * 3))
    y.append(0)  # Label for wave

    # Simulate stop gesture
    X.append(np.random.rand(21 * 3))
    y.append(1)  # Label for stop

X = np.array(X)
y = np.array(y)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train SVM classifier
classifier = SVC(kernel='linear')
classifier.fit(X_train, y_train)

# Evaluate accuracy
y_pred = classifier.predict(X_test)
print(f'Accuracy: {accuracy_score(y_test, y_pred)}')

# Start video capture
cap = cv2.VideoCapture(0)

print("Starting webcam (press Ctrl+C to stop)...")

try:
    while True:
        ret, frame = cap.read()
        if not ret:
            print("Failed to capture frame.")
            break
            frame = cv2.flip(frame, 1)  # Optional: flip for mirror view
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(rgb_frame)

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                features = extract_features(hand_landmarks.landmark).reshape(1, -1)
                gesture = classifier.predict(features)
                gesture_text = "Wave" if gesture[0] == 0 else "Stop"

                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                cv2.putText(frame, gesture_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,
                            1, (0, 255, 0), 2)

        # Display the frame in Colab
        clear_output(wait=True)
        cv2_imshow(frame)

        # Add a small delay
        time.sleep(0.1)

except KeyboardInterrupt:
    print("Stopped by user.")

# Release the capture
cap.release()